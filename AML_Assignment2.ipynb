{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgaGlr/NyB1Kg4mY03Kx6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarandeepSinghBedi/Applied_Machine_Learning/blob/main/AML_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "G-Gz8Is4kdwl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to load datasets"
      ],
      "metadata": {
        "id": "vqICv8LMksPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets():\n",
        "    # Load Banknote Authentication dataset\n",
        "    banknote_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "    banknote_cols = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
        "    try:\n",
        "        banknote_df = pd.read_csv(banknote_url, header=None, names=banknote_cols)\n",
        "    except:\n",
        "        print(\"Could not load from URL. Loading from backup list.\")\n",
        "        # Backup data loading with sample data\n",
        "        banknote_data = [\n",
        "            [3.6216,8.6661,-2.8073,-0.44699,0],\n",
        "            [4.5459,8.1674,-2.4586,-1.4621,0],\n",
        "            [3.866,-2.6383,1.9242,0.10645,0],\n",
        "            [3.4566,9.5228,-4.0112,-3.5944,0],\n",
        "            [0.32924,-4.4552,4.5718,-0.9888,0],\n",
        "            [-1.3971,3.2689,-4.9101,-1.5325,1],\n",
        "            [-1.6677,2.8399,-4.8663,-1.4192,1],\n",
        "            [-2.2588,4.3468,-6.2708,-1.9578,1],\n",
        "            [-2.7338,3.0997,-4.3117,-1.9329,1],\n",
        "            [-2.5439,3.5906,-4.6242,-1.9535,1]\n",
        "        ]\n",
        "        banknote_df = pd.DataFrame(banknote_data, columns=banknote_cols)\n",
        "\n",
        "    # Load Haberman's Survival dataset\n",
        "    haberman_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
        "    haberman_cols = ['age', 'operation_year', 'positive_nodes', 'survival_status']\n",
        "    try:\n",
        "        haberman_df = pd.read_csv(haberman_url, header=None, names=haberman_cols)\n",
        "    except:\n",
        "        print(\"Could not load from URL. Loading from backup list.\")\n",
        "        # Backup data loading with sample data\n",
        "        haberman_data = [\n",
        "            [30, 64, 1, 1],\n",
        "            [30, 62, 3, 1],\n",
        "            [30, 65, 0, 1],\n",
        "            [31, 59, 2, 1],\n",
        "            [31, 65, 4, 1],\n",
        "            [33, 58, 10, 1],\n",
        "            [33, 60, 0, 1],\n",
        "            [34, 59, 0, 2],\n",
        "            [34, 66, 9, 2],\n",
        "            [38, 69, 21, 2]\n",
        "        ]\n",
        "        haberman_df = pd.DataFrame(haberman_data, columns=haberman_cols)\n",
        "\n",
        "    # Converting survival status to 0 and 1 in Haberman dataset (if needed)\n",
        "    if haberman_df['survival_status'].max() == 2:\n",
        "        haberman_df['survival_status'] = haberman_df['survival_status'].map({1: 0, 2: 1})\n",
        "\n",
        "    return banknote_df, haberman_df"
      ],
      "metadata": {
        "id": "4LJXDBh0khJi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to explore datasets"
      ],
      "metadata": {
        "id": "577YrHM_kwW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_dataset(df, dataset_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nDataset shape:\", df.shape)\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    print(\"\\nClass distribution:\")\n",
        "    target_col = df.columns[-1]\n",
        "    print(df[target_col].value_counts())\n",
        "    print(f\"Class balance: {df[target_col].value_counts(normalize=True).round(3)}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"\\nMissing values:\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "    else:\n",
        "        print(\"\\nNo missing values found.\")"
      ],
      "metadata": {
        "id": "sFcP-Rmuk6nu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to preprocess datasets"
      ],
      "metadata": {
        "id": "0utxx7-xk88o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(df):\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "VmNhgqgclH11"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to train and evaluate models"
      ],
      "metadata": {
        "id": "UpBiFEdllJgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DSaWw7d2aFzu"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_models(X_train, X_test, y_train, y_test, dataset_name):\n",
        "    results = {}\n",
        "\n",
        "    # Define models with hyperparameter grids\n",
        "    models = {\n",
        "        'Naive Bayes': {\n",
        "            'model': GaussianNB(),\n",
        "            'params': {}  # Gaussian NB has no hyperparameters to tune\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'model': LogisticRegression(max_iter=1000, random_state=42),\n",
        "            'params': {\n",
        "                'C': [0.01, 0.1, 1, 10, 100],\n",
        "                'solver': ['liblinear', 'lbfgs']\n",
        "            }\n",
        "        },\n",
        "        'SVM': {\n",
        "            'model': SVC(random_state=42),\n",
        "            'params': {\n",
        "                'C': [0.1, 1, 10],\n",
        "                'kernel': ['linear', 'rbf'],\n",
        "                'gamma': ['scale', 'auto', 0.1, 1]\n",
        "            }\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestClassifier(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [None, 10, 20],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Model Training and Evaluation for {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for name, model_info in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        # Hyperparameter tuning with cross-validation\n",
        "        if model_info['params']:\n",
        "            grid_search = GridSearchCV(\n",
        "                model_info['model'],\n",
        "                model_info['params'],\n",
        "                cv=cv,\n",
        "                scoring='f1_macro',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "            print(f\"Best parameters: {best_params}\")\n",
        "        else:\n",
        "            best_model = model_info['model']\n",
        "            best_model.fit(X_train, y_train)\n",
        "            best_params = \"N/A\"\n",
        "\n",
        "        # Make predictions on test set\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        precision = precision_score(y_test, y_pred, average='macro')\n",
        "        recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "        print(\"\\nPerformance metrics:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "        print(f\"Macro Precision: {precision:.4f}\")\n",
        "        print(f\"Macro Recall: {recall:.4f}\")\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # Generate classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'best_params': best_params,\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'confusion_matrix': cm,\n",
        "            'model': best_model\n",
        "        }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to visualize results"
      ],
      "metadata": {
        "id": "clnzrb_mlu38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(results, dataset_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Results Visualization for {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    models = list(results.keys())\n",
        "    accuracies = [results[model]['accuracy'] for model in models]\n",
        "    f1_scores = [results[model]['f1'] for model in models]\n",
        "    precisions = [results[model]['precision'] for model in models]\n",
        "    recalls = [results[model]['recall'] for model in models]\n",
        "\n",
        "\n",
        "\n",
        "    # Print results in a summary table\n",
        "    results_df = pd.DataFrame({\n",
        "        'Model': models,\n",
        "        'Accuracy': accuracies,\n",
        "        'F1-Score': f1_scores,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls\n",
        "    })\n",
        "\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Find the best model based on F1-score\n",
        "    best_model_idx = np.argmax(f1_scores)\n",
        "    print(f\"\\nBest model for {dataset_name} based on F1-score: {models[best_model_idx]}\")\n",
        "    print(f\"Best F1-score: {f1_scores[best_model_idx]:.4f}\")\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "tvWxgrCulVaB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main\n"
      ],
      "metadata": {
        "id": "Rq19HS4yli6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======================================================\")\n",
        "print(\"UCI Machine Learning Repository - Classification Analysis\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "# Load datasets\n",
        "banknote_df, haberman_df = load_datasets()\n",
        "\n",
        "# Explore datasets\n",
        "print(\"\\nLoading datasets...\")\n",
        "explore_dataset(banknote_df, \"Banknote Authentication\")\n",
        "explore_dataset(haberman_df, \"Haberman's Survival\")\n",
        "\n",
        "# Dictionary to store all results\n",
        "all_results = {}\n",
        "summary_dfs = {}\n",
        "\n",
        "# Process Banknote Authentication dataset\n",
        "print(\"\\nProcessing Banknote Authentication dataset...\")\n",
        "X_train, X_test, y_train, y_test = preprocess_dataset(banknote_df)\n",
        "banknote_results = train_evaluate_models(X_train, X_test, y_train, y_test, \"Banknote Authentication\")\n",
        "summary_dfs[\"Banknote\"] = visualize_results(banknote_results, \"Banknote Authentication\")\n",
        "all_results[\"Banknote\"] = banknote_results\n",
        "\n",
        "# Process Haberman's Survival dataset\n",
        "print(\"\\nProcessing Haberman's Survival dataset...\")\n",
        "X_train, X_test, y_train, y_test = preprocess_dataset(haberman_df)\n",
        "haberman_results = train_evaluate_models(X_train, X_test, y_train, y_test, \"Haberman's Survival\")\n",
        "summary_dfs[\"Haberman\"] = visualize_results(haberman_results, \"Haberman's Survival\")\n",
        "all_results[\"Haberman\"] = haberman_results\n",
        "\n",
        "# Compare performances across datasets\n",
        "print(f\"\\n{'#'*50}\")\n",
        "print(\"\\nComparison of model performances across datasets:\")\n",
        "for model in summary_dfs[\"Banknote\"][\"Model\"]:\n",
        "  print(f\"\\n{model}:\")\n",
        "  print(f\"  Banknote F1-Score: {summary_dfs['Banknote'].loc[summary_dfs['Banknote']['Model'] == model, 'F1-Score'].values[0]:.4f}\")\n",
        "  print(f\"  Haberman F1-Score: {summary_dfs['Haberman'].loc[summary_dfs['Haberman']['Model'] == model, 'F1-Score'].values[0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKwHCek8kbKD",
        "outputId": "8d2e4a75-50c2-4c91-89ef-68638140f989"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================\n",
            "UCI Machine Learning Repository - Classification Analysis\n",
            "======================================================\n",
            "\n",
            "Loading datasets...\n",
            "\n",
            "==================================================\n",
            "Dataset: Banknote Authentication\n",
            "==================================================\n",
            "\n",
            "First 5 rows:\n",
            "   variance  skewness  curtosis  entropy  class\n",
            "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
            "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
            "2   3.86600   -2.6383    1.9242  0.10645      0\n",
            "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
            "4   0.32924   -4.4552    4.5718 -0.98880      0\n",
            "\n",
            "Dataset shape: (1372, 5)\n",
            "\n",
            "Data types:\n",
            "variance    float64\n",
            "skewness    float64\n",
            "curtosis    float64\n",
            "entropy     float64\n",
            "class         int64\n",
            "dtype: object\n",
            "\n",
            "Basic statistics:\n",
            "          variance     skewness     curtosis      entropy        class\n",
            "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
            "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
            "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
            "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
            "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
            "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
            "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
            "max       6.824800    12.951600    17.927400     2.449500     1.000000\n",
            "\n",
            "Class distribution:\n",
            "class\n",
            "0    762\n",
            "1    610\n",
            "Name: count, dtype: int64\n",
            "Class balance: class\n",
            "0    0.555\n",
            "1    0.445\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "No missing values found.\n",
            "\n",
            "==================================================\n",
            "Dataset: Haberman's Survival\n",
            "==================================================\n",
            "\n",
            "First 5 rows:\n",
            "   age  operation_year  positive_nodes  survival_status\n",
            "0   30              64               1                0\n",
            "1   30              62               3                0\n",
            "2   30              65               0                0\n",
            "3   31              59               2                0\n",
            "4   31              65               4                0\n",
            "\n",
            "Dataset shape: (306, 4)\n",
            "\n",
            "Data types:\n",
            "age                int64\n",
            "operation_year     int64\n",
            "positive_nodes     int64\n",
            "survival_status    int64\n",
            "dtype: object\n",
            "\n",
            "Basic statistics:\n",
            "              age  operation_year  positive_nodes  survival_status\n",
            "count  306.000000      306.000000      306.000000       306.000000\n",
            "mean    52.457516       62.852941        4.026144         0.264706\n",
            "std     10.803452        3.249405        7.189654         0.441899\n",
            "min     30.000000       58.000000        0.000000         0.000000\n",
            "25%     44.000000       60.000000        0.000000         0.000000\n",
            "50%     52.000000       63.000000        1.000000         0.000000\n",
            "75%     60.750000       65.750000        4.000000         1.000000\n",
            "max     83.000000       69.000000       52.000000         1.000000\n",
            "\n",
            "Class distribution:\n",
            "survival_status\n",
            "0    225\n",
            "1     81\n",
            "Name: count, dtype: int64\n",
            "Class balance: survival_status\n",
            "0    0.735\n",
            "1    0.265\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "No missing values found.\n",
            "\n",
            "Processing Banknote Authentication dataset...\n",
            "\n",
            "==================================================\n",
            "Model Training and Evaluation for Banknote Authentication\n",
            "==================================================\n",
            "\n",
            "Training Naive Bayes...\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.8641\n",
            "Macro F1-Score: 0.8620\n",
            "Macro Precision: 0.8630\n",
            "Macro Recall: 0.8613\n",
            "\n",
            "Confusion Matrix:\n",
            "[[203  26]\n",
            " [ 30 153]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88       229\n",
            "           1       0.85      0.84      0.85       183\n",
            "\n",
            "    accuracy                           0.86       412\n",
            "   macro avg       0.86      0.86      0.86       412\n",
            "weighted avg       0.86      0.86      0.86       412\n",
            "\n",
            "\n",
            "Training Logistic Regression...\n",
            "Best parameters: {'C': 100, 'solver': 'liblinear'}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.9903\n",
            "Macro F1-Score: 0.9902\n",
            "Macro Precision: 0.9893\n",
            "Macro Recall: 0.9913\n",
            "\n",
            "Confusion Matrix:\n",
            "[[225   4]\n",
            " [  0 183]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       229\n",
            "           1       0.98      1.00      0.99       183\n",
            "\n",
            "    accuracy                           0.99       412\n",
            "   macro avg       0.99      0.99      0.99       412\n",
            "weighted avg       0.99      0.99      0.99       412\n",
            "\n",
            "\n",
            "Training SVM...\n",
            "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 1.0000\n",
            "Macro F1-Score: 1.0000\n",
            "Macro Precision: 1.0000\n",
            "Macro Recall: 1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[229   0]\n",
            " [  0 183]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       229\n",
            "           1       1.00      1.00      1.00       183\n",
            "\n",
            "    accuracy                           1.00       412\n",
            "   macro avg       1.00      1.00      1.00       412\n",
            "weighted avg       1.00      1.00      1.00       412\n",
            "\n",
            "\n",
            "Training Random Forest...\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.9951\n",
            "Macro F1-Score: 0.9951\n",
            "Macro Precision: 0.9946\n",
            "Macro Recall: 0.9956\n",
            "\n",
            "Confusion Matrix:\n",
            "[[227   2]\n",
            " [  0 183]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       229\n",
            "           1       0.99      1.00      0.99       183\n",
            "\n",
            "    accuracy                           1.00       412\n",
            "   macro avg       0.99      1.00      1.00       412\n",
            "weighted avg       1.00      1.00      1.00       412\n",
            "\n",
            "\n",
            "==================================================\n",
            "Results Visualization for Banknote Authentication\n",
            "==================================================\n",
            "\n",
            "Summary of Results:\n",
            "              Model  Accuracy  F1-Score  Precision   Recall\n",
            "        Naive Bayes  0.864078  0.862046   0.862997 0.861264\n",
            "Logistic Regression  0.990291  0.990189   0.989305 0.991266\n",
            "                SVM  1.000000  1.000000   1.000000 1.000000\n",
            "      Random Forest  0.995146  0.995090   0.994595 0.995633\n",
            "\n",
            "Best model for Banknote Authentication based on F1-score: SVM\n",
            "Best F1-score: 1.0000\n",
            "\n",
            "Processing Haberman's Survival dataset...\n",
            "\n",
            "==================================================\n",
            "Model Training and Evaluation for Haberman's Survival\n",
            "==================================================\n",
            "\n",
            "Training Naive Bayes...\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.6848\n",
            "Macro F1-Score: 0.4884\n",
            "Macro Precision: 0.5067\n",
            "Macro Recall: 0.5037\n",
            "\n",
            "Confusion Matrix:\n",
            "[[60  8]\n",
            " [21  3]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.88      0.81        68\n",
            "           1       0.27      0.12      0.17        24\n",
            "\n",
            "    accuracy                           0.68        92\n",
            "   macro avg       0.51      0.50      0.49        92\n",
            "weighted avg       0.62      0.68      0.64        92\n",
            "\n",
            "\n",
            "Training Logistic Regression...\n",
            "Best parameters: {'C': 1, 'solver': 'liblinear'}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.6957\n",
            "Macro F1-Score: 0.4949\n",
            "Macro Precision: 0.5220\n",
            "Macro Recall: 0.5110\n",
            "\n",
            "Confusion Matrix:\n",
            "[[61  7]\n",
            " [21  3]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81        68\n",
            "           1       0.30      0.12      0.18        24\n",
            "\n",
            "    accuracy                           0.70        92\n",
            "   macro avg       0.52      0.51      0.49        92\n",
            "weighted avg       0.63      0.70      0.65        92\n",
            "\n",
            "\n",
            "Training SVM...\n",
            "Best parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.6957\n",
            "Macro F1-Score: 0.4949\n",
            "Macro Precision: 0.5220\n",
            "Macro Recall: 0.5110\n",
            "\n",
            "Confusion Matrix:\n",
            "[[61  7]\n",
            " [21  3]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81        68\n",
            "           1       0.30      0.12      0.18        24\n",
            "\n",
            "    accuracy                           0.70        92\n",
            "   macro avg       0.52      0.51      0.49        92\n",
            "weighted avg       0.63      0.70      0.65        92\n",
            "\n",
            "\n",
            "Training Random Forest...\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "\n",
            "Performance metrics:\n",
            "Accuracy: 0.6957\n",
            "Macro F1-Score: 0.5165\n",
            "Macro Precision: 0.5417\n",
            "Macro Recall: 0.5245\n",
            "\n",
            "Confusion Matrix:\n",
            "[[60  8]\n",
            " [20  4]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.88      0.81        68\n",
            "           1       0.33      0.17      0.22        24\n",
            "\n",
            "    accuracy                           0.70        92\n",
            "   macro avg       0.54      0.52      0.52        92\n",
            "weighted avg       0.64      0.70      0.66        92\n",
            "\n",
            "\n",
            "==================================================\n",
            "Results Visualization for Haberman's Survival\n",
            "==================================================\n",
            "\n",
            "Summary of Results:\n",
            "              Model  Accuracy  F1-Score  Precision   Recall\n",
            "        Naive Bayes  0.684783  0.488399   0.506734 0.503676\n",
            "Logistic Regression  0.695652  0.494902   0.521951 0.511029\n",
            "                SVM  0.695652  0.494902   0.521951 0.511029\n",
            "      Random Forest  0.695652  0.516517   0.541667 0.524510\n",
            "\n",
            "Best model for Haberman's Survival based on F1-score: Random Forest\n",
            "Best F1-score: 0.5165\n",
            "\n",
            "##################################################\n",
            "\n",
            "Comparison of model performances across datasets:\n",
            "\n",
            "Naive Bayes:\n",
            "  Banknote F1-Score: 0.8620\n",
            "  Haberman F1-Score: 0.4884\n",
            "\n",
            "Logistic Regression:\n",
            "  Banknote F1-Score: 0.9902\n",
            "  Haberman F1-Score: 0.4949\n",
            "\n",
            "SVM:\n",
            "  Banknote F1-Score: 1.0000\n",
            "  Haberman F1-Score: 0.4949\n",
            "\n",
            "Random Forest:\n",
            "  Banknote F1-Score: 0.9951\n",
            "  Haberman F1-Score: 0.5165\n"
          ]
        }
      ]
    }
  ]
}